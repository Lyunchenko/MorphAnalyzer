{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический анализ\n",
    "Хранение слов в обратном порядке букв, генерация незнакомых слов из похожих существующих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from difflib import SequenceMatcher as sm\n",
    "import json\n",
    "import shelve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частота лемм для устранения противоречий\n",
    "Источник размеченых текстов: http://opencorpora.org/?page=downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountWords:\n",
    "    '''Частота лемм в размеченых текстах'''\n",
    "    \n",
    "    map = {'adjf':'A', 'adjs':'A', 'advb':'ADV', 'comp':'ADV', 'conj':'CONJ', 'grnd':'V', 'infn':'V',\n",
    "           'intj':'ADV', 'latn':'NI', 'noun':'S', 'npro':'NI', 'pnct':'NI', 'prcl':'ADV', 'prtf':'V',\n",
    "           'prts':'V', 'romn':'NI', 'symb':'NI','verb':'V', 'prep':'PR', 'pred':'ADV', 'numr':'A'}\n",
    "    \n",
    "    file_name = '../data/words_count.shelve'\n",
    "    \n",
    "    def __init__(self, file_count):\n",
    "        if os.path.exists(self.file_name+'.dat'):\n",
    "            self._load_in_dump()\n",
    "            return\n",
    "        self.words_count = {}\n",
    "        self._parse_file(file_count)\n",
    "        self._save_to_dump()\n",
    "    \n",
    "    def get_count(self, lemma, g):\n",
    "        if (lemma in self.words_count and g in self.words_count[lemma]):\n",
    "            return self.words_count[lemma][g]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def _parse_file(self, file_count):\n",
    "        if not os.path.exists(file_count): raise\n",
    "        file = open(file_count, 'r', encoding='utf-8')\n",
    "        for line in file:\n",
    "            val = self._get_val(line)\n",
    "            if not val: continue\n",
    "            token, lemma, g = val\n",
    "            self._set_value(token, lemma, g)\n",
    "        file.close()\n",
    "    \n",
    "    def _set_value(self, token, lemma, g):\n",
    "        if not g in self.map: return\n",
    "        g = self.map[g]\n",
    "        if not lemma in self.words_count: self.words_count[lemma]={}\n",
    "        if not g in self.words_count[lemma]: self.words_count[lemma][g]=1\n",
    "        else: self.words_count[lemma][g] += 1\n",
    "    \n",
    "    def _get_val(self, line):\n",
    "        soup = BeautifulSoup(line, 'lxml').find('token')\n",
    "        if soup is None: return False\n",
    "        token = soup.get('text').lower()\n",
    "        lemma = soup.find('l').get('t').lower()\n",
    "        g = soup.find('g').get('v').lower()\n",
    "        if lemma is None: return False\n",
    "        return token, lemma, g\n",
    "    \n",
    "    def _save_to_dump(self):\n",
    "        end_file_name = ['.bak','.dat', '.dir']\n",
    "        for efn in end_file_name:\n",
    "            if os.path.exists(self.file_name+efn): \n",
    "                os.remove(self.file_name+efn)      \n",
    "        shelve.open(self.file_name).update(self.words_count)\n",
    "    \n",
    "    def _load_in_dump(self):\n",
    "        self.words_count = shelve.open(self.file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = CountWords('../data/annot.opcorpora.no_ambig.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнение словаря odict леммами из opcorpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADDLemm(CountWords):\n",
    "    \n",
    "    exclade_g = {'pnct', 'latn', 'unkn', 'numb', 'symb'}\n",
    "    \n",
    "    def __init__(self, file_words, file_count):\n",
    "        self.opcorpora = {}\n",
    "        self._parse_file(file_count)\n",
    "        self.odict = {}\n",
    "        self._parse_odict(file_words)\n",
    "        file = open('../data/odict.csv', 'a')\n",
    "        for lemma, v in self.opcorpora.items():\n",
    "            if not lemma in self.odict:\n",
    "                for ps, words in v.items():\n",
    "                    line = self._get_format_odict(lemma, ps, words)\n",
    "                    try:\n",
    "                        file.write(line + '\\n')\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        file.close()\n",
    "                \n",
    "    def _get_format_odict(self, lemma, ps, words):\n",
    "        line = lemma + ',' + ps\n",
    "        for word in words:\n",
    "            line += ','+word\n",
    "        return line\n",
    "    \n",
    "    def _set_value(self, token, lemma, g):\n",
    "        if len(lemma)<=1: return\n",
    "        if g in self.exclade_g: return\n",
    "        if not lemma in self.opcorpora: self.opcorpora[lemma]={}\n",
    "        if not g in self.opcorpora[lemma]: self.opcorpora[lemma][g] = {}\n",
    "        self.opcorpora[lemma][g][token] = 0\n",
    "    \n",
    "    def _parse_odict(self, file_words):\n",
    "        file = open(file_words)\n",
    "        for line in file:\n",
    "            l_arr = line[:-1].lower().split(sep=',')\n",
    "            self.odict[l_arr[0]] = 0\n",
    "        file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDLemm('../data/odict.csv', '../data/annot.opcorpora.no_ambig.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение словаря в дерево\n",
    "Источник словаря: http://odict.ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MorphAnalyzerFit:\n",
    "    \n",
    "    map = {'вводн.': 'ADV', 'ж': 'S','жо': 'S','м': 'S','межд.': 'ADV','мн.': 'S','мо': 'S',\n",
    "       'мо-жо': 'S','мс-п': 'A','н': 'ADV', 'нсв': 'V','п': 'A','предик.': 'ADV',\n",
    "       'предл.': 'PR','с': 'S','св': 'V','св-нсв': 'V','со': 'S','союз': 'CONJ',\n",
    "       'сравн.': 'A','част.': 'ADV','числ.': 'ADV','числ.-п': 'A',\n",
    "       'adjf':'A', 'adjs':'A', 'advb':'ADV', 'comp':'ADV', 'conj':'CONJ', 'grnd':'V', 'infn':'V',\n",
    "       'intj':'ADV', 'latn':'NI', 'noun':'S', 'npro':'NI', 'pnct':'NI', 'prcl':'ADV', 'prtf':'V',\n",
    "       'prts':'V', 'romn':'NI', 'symb':'NI','verb':'V', 'prep':'PR', 'pred':'ADV', 'numr':'A'}\n",
    "    \n",
    "    path = '../data/words/'\n",
    "        \n",
    "    def fit(self, file_words, file_count):\n",
    "        self.words_count = CountWords(file_count)\n",
    "        self.words = {}\n",
    "        file = open(file_words)\n",
    "        for line in file:\n",
    "            l_arr = line[:-1].lower().split(sep=',')\n",
    "            ps = self._get_ps(l_arr[1], l_arr[0])\n",
    "            if not ps: continue\n",
    "            self._set_words(l_arr[0], ps, l_arr[2:])\n",
    "        file.close()\n",
    "        del(self.words_count)\n",
    "        self._save_to_dump() \n",
    "    \n",
    "    def _get_ps(self, ps, words):\n",
    "        if not ps in self.map: \n",
    "            print(f'Ключ \"{ps}\" для слова \"{words}\" не размечен')\n",
    "            return False\n",
    "        return self.map[ps]\n",
    "    \n",
    "    def _set_words(self, lemma, ps, words):\n",
    "        words.append(lemma)\n",
    "        for word in words:\n",
    "            instruction = self._get_instruction(word, lemma, ps)\n",
    "            if ps=='V': l_count = len(words)\n",
    "            else: l_count = self.words_count.get_count(lemma, ps)\n",
    "            self._set_word(self.words, word, instruction, l_count)\n",
    "        \n",
    "    def _get_instruction(self, word, lemma, ps):\n",
    "        for i in range(len(lemma)):\n",
    "            if i>len(word)-1:\n",
    "                end_word, end_lemma = word[i:], lemma[i:]\n",
    "                break\n",
    "            if lemma[i]!=word[i]:\n",
    "                end_word, end_lemma = word[i:], lemma[i:]\n",
    "                break\n",
    "        else:\n",
    "            end_word, end_lemma = word[i+1:], lemma[i+1:]\n",
    "        return str(-len(end_word))+'|'+end_lemma+'|'+ps\n",
    "    \n",
    "    def _get_lemma(self, word, instruction):\n",
    "        l, end, ps = instruction.split('|')\n",
    "        l = int(l)\n",
    "        return (word if l==0 else word[:l])+end, ps\n",
    "    \n",
    "    def _set_word(self, words, word, instruction, l_count):\n",
    "        if len(word)==0: return\n",
    "        if not word[-1] in words: words[word[-1]]={}\n",
    "        words = words[word[-1]]\n",
    "        if len(word)==1:\n",
    "            if not 'val' in words: words['val']={instruction: l_count}\n",
    "            else: words['val'][instruction]=l_count\n",
    "        else:\n",
    "            self._set_word(words, word[:-1], instruction, l_count)\n",
    "    \n",
    "    def _save_to_dump(self):\n",
    "        for k, v in self.words.items():\n",
    "            file = open(self.path+k, 'w')\n",
    "            file.write(json.dumps(v))\n",
    "            file.close()\n",
    "\n",
    "    def _load_in_dump(self):\n",
    "        self.words = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            file = open(self.path+filename, 'r')\n",
    "            self.words[filename]=json.loads(file.read())\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск или генерация лемм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphAnalyzer(MorphAnalyzerFit):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._load_in_dump()\n",
    "    \n",
    "    def parse(self, word):\n",
    "        instructions = self._get_val_in_dict(self.words, word)\n",
    "        if not instructions: \n",
    "            variants = self._get_variants(self.words, word, '', -2)\n",
    "            if not variants: return word, 'ADV'\n",
    "            instructions = {}\n",
    "            for  v in variants:\n",
    "                v_rev=''\n",
    "                for ch in reversed(v): v_rev+=ch\n",
    "                leven = sm(None, word, v_rev).ratio()\n",
    "                if leven>=0.75:\n",
    "                    for k in variants[v]:\n",
    "                        if not k in instructions: instructions[k]=1\n",
    "                        else: instructions[k]+=1\n",
    "        val = {'instruction': False, 'count': -1}\n",
    "        for k, v in instructions.items():\n",
    "            if v>val['count']:\n",
    "                val['count'] = v\n",
    "                val['instruction'] = k\n",
    "        if not val['instruction']: return word, 'ADV'\n",
    "        return self._get_lemma(word, val['instruction'])\n",
    "        \n",
    "    \n",
    "    def _get_val_in_dict(self, words, word):\n",
    "        if len(word)==0: return False\n",
    "        if word[-1] in words: words = words[word[-1]]\n",
    "        else: return False\n",
    "        if len(word)==1:\n",
    "            if 'val' in words: return words['val']\n",
    "            return False\n",
    "        return self._get_val_in_dict(words, word[:-1])\n",
    "    \n",
    "    def _get_variants(self, words, word, w, step):\n",
    "        if len(word)==0: return False\n",
    "        if not word[-1] in words: \n",
    "            if step>=0: return self._get_all(words, w, {})\n",
    "            else: return False\n",
    "        w+=word[-1]\n",
    "        words = words[word[-1]]\n",
    "        if step>=0: return self._get_all(words, w, {})\n",
    "        if len(word)==1: return False\n",
    "        return self._get_variants(words, word[:-1], w, step+1)\n",
    "\n",
    "    def _get_all(self, words, w, instructions):\n",
    "        for k in words:\n",
    "            if k=='val': instructions.update({w: words['val']})\n",
    "            else: instructions.update(self._get_all(words[k], w+k, instructions))\n",
    "        return instructions\n",
    "    \n",
    "    def _get_value(self):\n",
    "        pass\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполнение тестового задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = MorphAnalyzer()\n",
    "# obj.fit('../data/odict.csv', '../data/annot.opcorpora.no_ambig.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = open('dataset_37845_1.txt', 'r', encoding='utf-8')\n",
    "file_out = open('answer.txt', 'w', encoding='utf-8')\n",
    "for line in file_in:\n",
    "    text_in = line.replace(',','').replace('.','').replace('?','').replace('!','').replace('\\n',' ')\n",
    "    text_out = ''\n",
    "    for word in text_in.split(' '):\n",
    "        if len(word)==0: continue\n",
    "        p = obj.parse(word.lower())\n",
    "        val = word+'{'+p[0]+'='+p[1]+'}'\n",
    "        text_out = val if text_out=='' else text_out+' '+val\n",
    "    file_out.write(text_out + '\\n')\n",
    "file_in.close()\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"stepik.jpg\"><center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
